In this chapter, we discuss the distribution functions of discrete and continuous variables as well as the different forms of distribution. For the expected values and the variance of the different distributions look here: [[Expected Value & Variance]].
## Distribution Functions
Distribution functions answer the questions: *What is the probability of a specific value or range of values occurring?* and *What is the probability that the value is less than or equal to a certain number?*
### Probability Mass Function for Discrete Random Variables
The probability mass function (PMF) gives the probability of each possible outcome for aÂ **discrete**Â random variable ğ¹ âˆ¶ ğ‘† â†’ \[0,1], it gives the probability $p(ğ‘¥) = ğ‘ƒ(ğ‘‹ = ğ‘¥)$. It answers the question: **What is the probability of a specific value occurring?**
### Cumulative Distribution Function for Discrete Random Variables
The cumulative distribution function (CDF) of **discrete** random variables  
ğ¹ âˆ¶ ğ‘† â†’ \[0,1], it  gives the probability ğ¹(ğ‘¥) = ğ‘ƒ(ğ‘‹ â‰¤ ğ‘¥). It answers the question: **What is the probability that the value is less than or equal to a certain number?**

Example:
Let the random variable $X$ map the sums of two fair dice to their probability. 

![[Pasted image 20240924103004.png]]
Then for PMF, $P(X = 7) = 1/6$ and for CDF, $P(X <= 3) = 1/2$
### Probability Density Function for Continuous Random Variables
The probability density function gives us the probability that the possible outcome lies in a range from $a$ to $b$ for a **continuous** random variable $ğ‘“ âˆ¶ ğ‘† â†’ [0, âˆ)$, it gives the probability $ğ‘ƒ(ğ‘ â‰¤ ğ‘¥ â‰¤ ğ‘) = \int_a^b ğ‘“(ğ‘¡)dğ‘¡$. *Important:* the probability of any given value $P(a = x)$ is always $0$.

The PDF must satisfy two conditions:
- **Non-negativity**:Â  $f(x) \geq 0$ for all $x$ in the domain.
- **Total probability equals 1**: The integral of $f(x)$ over the entire domain must equal 1.
### Cumulative Distribution Function for Continuous Random Variables
The cumulative distribution function gives us the probability that the possible outcome is smaller or equal to $x$ range from $a$ to $b$ for a **continuous** random variable $ğ‘“ âˆ¶ ğ‘† â†’ [0, âˆ)$, it gives the probability $ğ‘ƒ(X â‰¤ x) = \int_{-\infty}^x ğ‘“(ğ‘¡)dğ‘¡$. 
## Different Forms of Distribution
The different forms of distribution tell us what we can expect the outcome to be. Are the outcomes integers from a...b, are the outcomes "yes" or "no", are the outcomes multiple "yes" or "no's" (an outcome could be: 2 "yes's"), ... It basically tells us what $X$ will look like.
### Discrete Uniform Distribution for Discrete Random Variables
Suppose $X$ takes its values from $n$ integers $a ... b$ such that each integer is equally likely, that is, $ğ‘ƒ (ğ‘‹ = ğ‘˜) = 1/n$ for all ğ‘˜. Then ğ‘‹ has a discrete uniform distribution, denoted ğ‘‹ âˆ¼ ğ‘ˆ{ğ‘, ğ‘}. The PMF of the discrete uniform distribution looks something like this: 
![[Pasted image 20241030141049.png|400]]
### Bernoulli Distribution for Discrete Random Variables
Bernoulli variables describe the outcome of a single experiment that can be answered â€œyesâ€ or â€œnoâ€, that is, that behaves like a Boolean value. Bernoulli distribution is denoted $ğ‘‹ âˆ¼ Bernoulli(ğ‘).$

Example, let us throw two fair dice:  
- Let ğ‘‹ be an indicator variable for the event that the sum of the face values is equal to 8  
- There are 5 outcomes with this property out of 36  
- Thus $ğ‘‹ âˆ¼ Bernoulli(ğ‘)$ with $ğ‘ = 5/36$
- PMF: $P(X = 1) = 5/36$ and $P(X = 0) = 31/36$
- CDF: 
	- For $x < 0: F(x) = 0$
	- For $0 <= x < 1: F(x) = 31/36$
	- For $x >= 1: F(x) = 1$
- The PMF would look something like this: ![[Pasted image 20241030141554.png|400]]
### Categorical Distribution for Discrete Random Variables
The categorical distribution for a random variable $X$Â  that can take $k$ different categories is typically denoted as: $X \sim \text{Categorical}(p_1, p_2, \dots, p_k)$. This just represents a Bernoulli experiment with more than two possible outcomes. 

Example, suppose you have a categorical distribution with categories (e.g., â€œRedâ€, â€œGreenâ€, â€œBlueâ€) and their respective probabilities.
- $X \sim \text{Categorical}(p_1, p_2, p_3)$, with $p_1 = 0.2, p_2 = 0.5, p_3 = 0.3$ 
- The PMF would look like this:                    ![[Pasted image 20241111133926.png|400]]
### Binomial Distribution for Discrete Random Variables
Let ğ‘‹ be the sum of ğ‘› independent and identically distributed Bernoulli variables, that is, each $ğ‘‹ âˆ¼ Bernoulli(ğ‘)$ and they are independent. We then say that ğ‘‹ has a Binomial distribution, denoted $ğ‘‹ âˆ¼ Bin(ğ‘›, ğ‘)$

Example, let us toss a coin five times:
- $ğ‘‹ âˆ¼ Bin(ğ‘›, ğ‘)$ wich $n=10$ and $p=1/2$
- The probability of observing $k$ heads is: $(noverk) * (p)^k * (1-p)^{n-k}$
- The PMF would look something like this: ![[Pasted image 20241030141744.png|400]]
### Multinomial Distribution for Discrete Random Variables
The multinomial distribution is a generalization of the binomial distribution. It models the outcome of experiments in which each trial results in one of several discrete categories, but allows for multiple trials. We denote a multinomial distribution as $X \sim \text{Multinomial}(n, p_1, p_2, \dots, p_k)$

We cannot compute the PMF of all the outcomes in two dimensions, but we can plot the PMF for one of these outcomes. Then it just looks like the PMF of the Binomial distribution.
### Geometric Distribution for Discrete Random Variables
Suppose we perform Bernoulli trials with success probability ğ‘ until we observe the first success. Let ğ‘‹ be the number of trials until the first success, we then say that ğ‘‹ has geometric distribution, denoted $ğ‘‹ âˆ¼ Geom(ğ‘)$

Unfortunately, there are two different interpretations of this distribution which are both called geometric distribution. 
- probability distribution of the numberÂ $X$Â ofÂ Bernoulli trials needed to get one success. (Counts successes and failures)
- probability distribution of the numberÂ $Y=Xâˆ’1$ of failures before the first success. (Counts just failures)

The PMF of a geometric distribution, in this example with $p=0.3$, could look something like this: ![[Pasted image 20241030142154.png|400]]

Example, It is estimated that approximately 105 boys are born per 100 girls. Suppose a couple decides to have as many kids as necessary until they have a girl. Let's find the probability that they need to have five kids (the fifth kid is the first â€œsuccessâ€):![[Pasted image 20240924112912.png|400]]

We can also interpret the question as "We want to find out the probability that we have 4 failures to have a girl". Then we would only count failures and the probability would be calculated like so: $(1-p)^k*p$. But because $k$ in this case would be four and not five we would still get the same result.
### Poisson Distribution for Discrete Random Variables
Suppose events happen in a fixed interval of time or space at a fixed rate independently, what is the number of events ğ‘‹? We say ğ‘‹ follows the Poisson distribution with rate ğœ†, denoted $ğ‘‹ âˆ¼ Poisson(ğœ†)$. The PMF can be calculated as follows: 
![[Pasted image 20240924113709.png|200]]

Example, suppose there are on average 20 visitors to a webpage in an hour, what is the probability that 21 visitors visit the webpage assuming Poisson distribution: 
- $P(X = 21) = {20^{21} * e^{-20} \over 21!}$
- $P(X = 21) â‰ˆ 0.0847$

The PMF of this example looks like this: 
![[Pasted image 20241030142410.png|400]]
We can see that this looks very similar to the binomial distribution. This is because the Poisson distribution is an approximation for the Binomial distribution in cases of large $n$ and smallÂ $p$.
### Uniform Continuous Distribution for Continuous Random Variables
A uniform continuous distribution is a probability distribution where all outcomes within a given range are equally likely. The probability density function (PDF) of a uniform distribution is constant within a specified interval $[a, b]$ and zero outside that interval.

Let's look at a example where $a=9$ and $b=10$:
- The PDF is 
	- $1 \over b - a$ for $a \leq x \leq b$ 
	- $0$ everywhere else
- The CDF is 
	- $0$ for $x<a$
	- $x-a \over b-a$ for $a \leq x \leq b$ 
	- 1 for $x>b$
- For example the probability of $9.5$ is $0$, because in continuous distributions, the probability of any specific point is 0.
- For example the probability of $x$ being between $9$ and $9.5$ is $P(9 \leq X \leq 9.5) = \int_9^{9.5} 1 \, dx = 9.5 - 9 = 0.5$
- The PDF in this case would look something like this: ![[Pasted image 20241030143316.png|400]]
### Exponential Distribution for Continuous Random Variables
The exponential distribution is the continuous analogue of the geometric distribution. It models the amount of time (or distance) you need to wait for an event to happen. When the exponential distribution is denoted as $ğ‘‹ âˆ¼ Exp(ğœ†)$ we say that $X$ is distributed with the rate $ğœ†$. For example, ifÂ  $\lambda = 0.5$, it means, on average, one event occurs every 2 time units (since the rate of 0.5 events per time unit indicates 1 event every $\frac{1}{0.5} = 2$Â units of time). The PDF of the exponential distribution looks something like this:
![[Pasted image 20241030143400.png|400]]
### Normal Distribution for Continuous Random Variables
The normal distribution is the familiar bell curve: A lot of natural and social phenomena can be modeled with the normal distribution. It is denoted as $ğ‘‹ âˆ¼ ğ’©(ğœ‡, ğœ^2)$. The normal distribution is defined by the parameters ğœ‡, the mean of the distribution, and ğœ, the standard deviation. If $ğœ‡ = 0$ and $ğœ = 1$, we say the resulting distribution is the standard normal distribution. 

The normal distribution satisfies the *68%â€“95%â€“99%* rule. Sixty-eight percent of the probability mass must lie within the region $Â±1Ïƒ$ of the mean. Further, 95% of the probability is within $2Ïƒ$, and 99.7% within $3Ïƒ$.

The PDF of the normal distribution looks something like this:
![[Pasted image 20241030143440.png|400]]
### Power Law Distribution for Continuous Random Variables
The Power Law distribution is used to describe a wide range of phenomena in social, biological, and physical systems where extreme values play a significant role. It is characterized by *preferential growth*. This means that the richer get richer. An example is the wealth distribution. In the distribution that describes the wealth distribution there must be a probability that there exists a "Bill Gates" with 80 Billion dollars of wealth. 

While the exponential distribution is denoted as something where the exponent is a variable $x$, the power law distribution is defined as $P(x) = C x^{-\alpha}$. This leads to the special characteristic, where big events can still happen more often than expected. 

The similarity in the exponential distribution and the power law distribution can also be seen in the PDF of these two distributions. They look very similar but the power law distribution decreases more slowly and denotes a higher probability to large values: ![[Pasted image 20241030151117.png|400]]

