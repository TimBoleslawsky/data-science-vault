When you move from “ordinary” data science projects (like regression, decision trees, classical ML pipelines) to deep learning projects the project workflow outline in [[Data Science#The Goals of Data Science]] changes substantially. The high-level stages are similar (data → preprocessing → modeling → evaluation), but the **boundaries blur** and the emphasis shifts. 
## Data Munging
In classical data science, exploratory data analysis (EDA) is central and usually performed offline. Analysts explore distributions, correlations, missing values, and outliers, often aided by the fact that datasets are small enough for direct inspection. In deep learning, EDA remains important but takes a lighter form, since high-dimensional data like images or text cannot be fully examined in the same way. Instead, the focus is on sanity checks such as class balance, label distributions, sample visualizations, and data quality rather than detailed feature-level correlations, because the model itself is expected to learn representations.
## **Data Preprocessing**
In classical data science, preprocessing is mostly offline: scaling, imputing, encoding, and feature engineering are performed once to produce a clean dataset for modeling. Deep learning, by contrast, shifts much of preprocessing into the training phase, where normalization, resizing, tokenization, and stochastic data augmentation are applied on-the-fly each epoch. Basic cleaning and global statistics (e.g., mean/std) are still computed offline, but heavy feature engineering is generally avoided, as deep networks are designed to capture temporal or spatial dependencies directly. 

=> Important note for data preprocessing and data munging: *In structured domains like time series, preprocessing and EDA can remain as crucial as in classical ML, even if handcrafted lags and transformations are often replaced by learned representations!*
## **Modeling**
In classical data science, model building is relatively quick. You can train many models (logistic regression, random forest, XGBoost, etc.) in minutes and compare them. In deep learning, model development is **engineering-heavy**. You define architectures (CNNs, RNNs, Transformers, etc.) instead of choosing off-the-shelf models. Hyperparameters (layers, learning rate, batch size, optimizers, regularization) play a massive role. Model training is slow and requires GPUs or distributed computing.
## **Model Training**
This is where the biggest shift is. In classical data science, the model training part is usually just a simple fit. In deep learning, training is tightly coupled with preprocessing. Images, for example, are normalized and augmented _inside the training loop_.
## **Evaluation**
In classical data science, you evaluate on validation/test sets, often using cross-validation to compensate for small data. In deep learning, cross-validation is rare (too expensive). Instead, you use train/val/test splits or K-fold with smaller models. You often track performance over epochs, monitoring validation loss to detect overfitting (early stopping, learning rate scheduling). Model checkpoints, reproducibility, and experiment logging (TensorBoard, WandB) are crucial because training is costly.